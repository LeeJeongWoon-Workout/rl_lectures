{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled33.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Mujoco Installation in Colab \n",
        "!apt-get install -y \\\n",
        "    libgl1-mesa-dev \\\n",
        "    libgl1-mesa-glx \\\n",
        "    libglew-dev \\\n",
        "    libosmesa6-dev \\\n",
        "    software-properties-common\n",
        "!apt-get install -y patchelf\n",
        "!git clone https://github.com/LeeJeongWoon-Workout/deep_rl_algorithm.git\n",
        "!pip install gym\n",
        "!pip install free-mujoco-py\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install imageio==2.4.1\n",
        "!pip install -U colabgymrender"
      ],
      "metadata": {
        "id": "6ds3ulkX_VVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gym\n",
        "import time\n",
        "import argparse\n",
        "import datetime\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from os.path import join\n",
        "import deep_rl_algorithm \n",
        "import mujoco_py\n",
        "from colabgymrender.recorder import Recorder\n",
        "\n",
        "!cp -r deep_rl_algorithm/agents agents "
      ],
      "metadata": {
        "id": "7lQngJHWPdVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurations\n",
        "parser = argparse.ArgumentParser(description='RL algorithms with PyTorch in MuJoCo environments')\n",
        "parser.add_argument('--env', type=str, default='HalfCheetah-v2', \n",
        "                    help='choose an environment between Hopper-v2, HalfCheetah-v2, Ant-v2 and Humanoid-v2')\n",
        "parser.add_argument('--algo', type=str, default='sac', \n",
        "                    help='select an algorithm among dqn, vpg, trpo, ppo, ddpg, td3, sac')\n",
        "parser.add_argument('--phase', type=str, default='train',\n",
        "                    help='choose between training phase and testing phase')\n",
        "parser.add_argument('--render', action='store_true', default=False,\n",
        "                    help='if you want to render, set this to True')\n",
        "parser.add_argument('--load', type=str, default=None,\n",
        "                    help='copy & paste the saved model name, and load it')\n",
        "parser.add_argument('--seed', type=int, default=0, \n",
        "                    help='seed for random number generators')\n",
        "parser.add_argument('--iterations', type=int, default=50, \n",
        "                    help='iterations to run and train agent')\n",
        "parser.add_argument('--steps_per_iter', type=int, default=5000, \n",
        "                    help='steps of interaction for the agent and the environment in each epoch')\n",
        "parser.add_argument('--max_step', type=int, default=1000,\n",
        "                    help='max episode step')\n",
        "parser.add_argument('--tensorboard', action='store_true', default=True)\n",
        "parser.add_argument('--gpu_index', type=int, default=0)\n",
        "args = parser.parse_args(args=[])\n",
        "device = torch.device('cuda', index=args.gpu_index) if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "Ld56FUY1PdTd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FT0-OD2JS-AJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if args.algo=='a2c':\n",
        "  from agents.a2c import Agent\n",
        "elif args.algo=='ddpg':\n",
        "  from agents.ddpg import Agent\n",
        "elif args.algo=='dqn':\n",
        "  from agents.dqn import Agent\n",
        "elif args.algo=='ppo':\n",
        "  from agents.ppo import Agent\n",
        "elif args.algo=='sac':\n",
        "  from agents.sac import Agent\n",
        "elif args.algo=='td3':\n",
        "  from agents.td3 import Agent\n",
        "elif args.algo=='trpo':\n",
        "  from agents.trpo import Agent\n",
        "else:\n",
        " from agents.vpg import Agent"
      ],
      "metadata": {
        "id": "lA2rxFhJPdR1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main.\"\"\"\n",
        "\n",
        "    \n",
        "    # Initialize environment\n",
        "    env=gym.make(args.env)\n",
        "    obs_dim = env.observation_space.shape[0]\n",
        "    act_dim = env.action_space.shape[0]\n",
        "    act_limit = env.action_space.high[0]\n",
        "\n",
        "\n",
        "    print('---------------------------------------')\n",
        "    print('Environment:', args.env)\n",
        "    print('Algorithm:', args.algo)\n",
        "    print('State dimension:', obs_dim)\n",
        "    print('Action dimension:', act_dim)\n",
        "    print('Action limit:', act_limit)\n",
        "    print('---------------------------------------')\n",
        "\n",
        "    # Set a random seed\n",
        "    env.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "\n",
        "    # Create an agent\n",
        "    if args.algo == 'ddpg' or args.algo == 'td3':\n",
        "        agent = Agent(env, args, device, obs_dim, act_dim, act_limit, \n",
        "                      act_noise=0.1, \n",
        "                      hidden_sizes=(256,256), \n",
        "                      buffer_size=int(1e6), \n",
        "                      batch_size=64,\n",
        "                      policy_lr=3e-4, \n",
        "                      qf_lr=3e-4)\n",
        "    elif args.algo == 'sac':                                                                                    \n",
        "        agent = Agent(env, args, device, obs_dim, act_dim, act_limit,                                                \n",
        "                      alpha=0.2,                        # In HalfCheetah-v2 and Ant-v2, SAC with 0.2  \n",
        "                      hidden_sizes=(256,256),           # shows the best performance in entropy coefficient \n",
        "                      buffer_size=int(1e6),             # while, in Humanoid-v2, SAC with 0.05 shows the best performance.\n",
        "                      batch_size=64,\n",
        "                      policy_lr=3e-4, \n",
        "                      qf_lr=3e-4)\n",
        "\n",
        "    elif args.algo=='dqn':\n",
        "      agent=Agent(env,args,device,obs_dim,act_dim)     \n",
        "    else: # vpg,trpo,ppo\n",
        "        agent = Agent(env, args, device, obs_dim, act_dim, act_limit, sample_size=4096)\n",
        "\n",
        "    # If we have a saved model, load it\n",
        "    if args.load is not None:\n",
        "        pretrained_model_path = os.path.join('./save_model/' + str(args.load))\n",
        "        pretrained_model = torch.load(pretrained_model_path, map_location=device)\n",
        "        if args.algo=='dqn':\n",
        "          agent.qf.load_state_dict(pretrained_model)\n",
        "        else:\n",
        "          agent.policy.load_state_dict(pretrained_model)\n",
        "\n",
        "    # Create a SummaryWriter object by TensorBoard\n",
        "    if args.tensorboard and args.load is None:\n",
        "        dir_name = 'runs/' + args.env + '/' \\\n",
        "                           + args.algo \\\n",
        "                           + '_s_' + str(args.seed) \\\n",
        "                           + '_t_' + datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
        "        writer = SummaryWriter(log_dir=dir_name)\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    total_num_steps = 0\n",
        "    train_sum_returns = 0.\n",
        "    train_num_episodes = 0\n",
        "\n",
        "    # Main loop\n",
        "    for i in range(args.iterations):\n",
        "        # Perform the training phase, during which the agent learns\n",
        "        if args.phase=='test' and args.load is not None:\n",
        "          directory='./video'\n",
        "          agent.env=Recorder(agent.env,directory)\n",
        "          agent.run(args.max_step)\n",
        "          agent.env.play()\n",
        "          break\n",
        "\n",
        "        \n",
        "        else:\n",
        "            train_step_count = 0\n",
        "\n",
        "            while train_step_count <= args.steps_per_iter:\n",
        "                agent.eval_mode = False\n",
        "                \n",
        "                # Run one episode\n",
        "\n",
        "                train_step_length, train_episode_return = agent.run(args.max_step)\n",
        "                total_num_steps += train_step_length\n",
        "                train_step_count += train_step_length\n",
        "                train_sum_returns += train_episode_return\n",
        "                train_num_episodes += 1\n",
        "\n",
        "                train_average_return = train_sum_returns / train_num_episodes if train_num_episodes > 0 else 0.0\n",
        "\n",
        "                # Log experiment result for training steps\n",
        "                if args.tensorboard and args.load is None:\n",
        "                    writer.add_scalar('Train/AverageReturns', train_average_return, total_num_steps)\n",
        "                    writer.add_scalar('Train/EpisodeReturns', train_episode_return, total_num_steps)\n",
        "\n",
        "\n",
        "        # Perform the evaluation phase -- no learning\n",
        "        eval_sum_returns = 0.\n",
        "        eval_num_episodes = 0\n",
        "        agent.eval_mode = True # When eval_mode is True, no parameter updating \n",
        "\n",
        "        for _ in range(10):\n",
        "            # Run one episode\n",
        "            eval_step_length, eval_episode_return = agent.run(args.max_step)\n",
        "\n",
        "            eval_sum_returns += eval_episode_return\n",
        "            eval_num_episodes += 1\n",
        "\n",
        "        eval_average_return = eval_sum_returns / eval_num_episodes if eval_num_episodes > 0 else 0.0\n",
        "\n",
        "        # Log experiment result for evaluation steps\n",
        "        if args.tensorboard and args.load is None:\n",
        "            writer.add_scalar('Eval/AverageReturns', eval_average_return, total_num_steps)\n",
        "            writer.add_scalar('Eval/EpisodeReturns', eval_episode_return, total_num_steps)\n",
        "\n",
        "        if args.phase == 'train':\n",
        "            print('---------------------------------------')\n",
        "            print('Iterations:', i + 1)\n",
        "            print('Steps:', total_num_steps)\n",
        "            print('Episodes:', train_num_episodes)\n",
        "            print('EpisodeReturn:', round(train_episode_return, 2))\n",
        "            print('AverageReturn:', round(train_average_return, 2))\n",
        "            print('EvalEpisodes:', eval_num_episodes)\n",
        "            print('EvalEpisodeReturn:', round(eval_episode_return, 2))\n",
        "            print('EvalAverageReturn:', round(eval_average_return, 2))\n",
        "            print('Time:', int(time.time() - start_time))\n",
        "            print('---------------------------------------')\n",
        "\n",
        "            # Save the trained model\n",
        "            if not os.path.exists('./save_model'):\n",
        "                os.mkdir('./save_model')\n",
        "            \n",
        "            ckpt_path = os.path.join('./save_model/' + args.env + '_' + args.algo \\\n",
        "                                                                + '_s_' + str(args.seed) \\\n",
        "                                                                + '_i_' + str(i + 1) \\\n",
        "                                                                + '_tr_' + str(round(train_episode_return, 2)) \\\n",
        "                                                                + '_er_' + str(round(eval_episode_return, 2)) + '.pt')\n",
        "            if args.algo=='dqn':\n",
        "              torch.save(agent.qf.state_dict(),ckpt_path)\n",
        "            else:\n",
        "              torch.save(agent.policy.state_dict(), ckpt_path)\n",
        "\n",
        "    \n"
      ],
      "metadata": {
        "id": "a6Fb970KPdPz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "bo-6MG2J_WFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_rcnv4NqvTqg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}